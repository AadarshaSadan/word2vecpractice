{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bajag\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import,division,print_function\n",
    "#for word encoding\n",
    "import codecs\n",
    "#regex\n",
    "import glob\n",
    "#concurrency\n",
    "import multiprocessing\n",
    "#dealin with operating system,like read file\n",
    "import os\n",
    "#import pprint\n",
    "#regular expression\n",
    "import re\n",
    "#naturaal lanuage toolkit\n",
    "import nltk\n",
    "#word 2 vec\n",
    "import gensim.models.word2vec as w2v\n",
    "#dimensionality reduction\n",
    "import sklearn.manifold\n",
    "#math\n",
    "import numpy as np\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#visualization \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bajag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bajag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#process our data\n",
    "#clan\n",
    "nltk.download('punkt')#pretrained tokenizer\n",
    "nltk.download('stopwords')#word like and,the,as ,an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found books\n"
     ]
    }
   ],
   "source": [
    "#get the book names mathing text file name\n",
    "book_filenames=sorted(glob.glob(\"data/*.txt\"))\n",
    "if book_filenames:\n",
    "    print(\"found books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine books into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readinfile'data\\got1.txt'...\n",
      "corpus is now 1770659 char long\n",
      "\n",
      "Readinfile'data\\got2.txt'...\n",
      "corpus is now 4071041 char long\n",
      "\n",
      "Readinfile'data\\got3.txt'...\n",
      "corpus is now 6391405 char long\n",
      "\n",
      "Readinfile'data\\got4.txt'...\n",
      "corpus is now 8107945 char long\n",
      "\n",
      "Readinfile'data\\got5.txt'...\n",
      "corpus is now 9719485 char long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_raw=u\"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Readinfile'{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename,\"r\",  \"utf-8\") as book_file:\n",
    "        corpus_raw+=book_file.read()\n",
    "    print(\"corpus is now {0} char long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split corpus into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#into sentences\n",
    "raw_sentences=tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sentence into list of words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean=re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words=clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#senteces where each words is tokinize\n",
    "sentences=[]\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence)>0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library of Congress Catalog Card Number: 98-37954.\n",
      "['Library', 'of', 'Congress', 'Catalog', 'Card', 'Number']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[6])\n",
    "print(sentence_to_wordlist(raw_sentences[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book contains 1,818,103\n"
     ]
    }
   ],
   "source": [
    "token_count=sum([len(sentence)for sentence in sentences])\n",
    "print(\"book contains {0:,}\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once we have vector our 3 main task taht vector help is\n",
    "#distance,similarity,ranking\n",
    "#more dimension more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more generalize\n",
    "\n",
    "#dimensionality of the resultinf word vector\n",
    "num_features=300\n",
    "\n",
    "#vector is a type of tensor\n",
    "\n",
    "#minimum word count threshold\n",
    "min_word_count=3\n",
    "\n",
    "#number of threads to run in paralle;\n",
    "#more worker faster to train\n",
    "num_workers=multiprocessing.cpu_count()\n",
    "\n",
    "#context window length\n",
    "context_size=7\n",
    "\n",
    "downsampling=1e-3\n",
    "\n",
    "#seed for the RNG,to make  the result reproductive\n",
    "#random number generator\n",
    "#deterministic,good for debugging\n",
    "seed=1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec=w2v.Word2Vec(\n",
    "    sentences,\n",
    "    #sg=1,\n",
    "    #seed=seed,\n",
    "    size=num_features,\n",
    "    window=context_size,\n",
    "    min_count=min_word_count,\n",
    "    workers=num_workers,\n",
    "    \n",
    "    #sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thrones2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordvec vocabulary length: 17277\n"
     ]
    }
   ],
   "source": [
    "print(\"wordvec vocabulary length:\",len(thrones2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14043790, 18181030)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones2vec.train(sentences,total_examples=len(sentences),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained_sad\"):\n",
    "    os.makedirs(\"trained_sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec.save(os.path.join(\"trained_sad\",\"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load trained module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec=w2v.Word2Vec.load(os.path.join(\"trained_sad\",\"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compress the word vectors into 2d space and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne=sklearn.manifold.TSNE(n_components=2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bajag\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_word_vectors_matrix=thrones2vec.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_vectors_matrix_2d=tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-112436e18642>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-112436e18642>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    points=pd.DataFrame{\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "points=pd.DataFrame{ \n",
    "    [\n",
    "        (word,coords[0],coords[1])\n",
    "        for word,coords in[\n",
    "            (word,all_word_vectors_matrix_2d[thrones2vec.vocab[word].index])\n",
    "            for word in thrones2vec.vocab\n",
    "        ]\n",
    "    ],\n",
    "    colums=[\"word\",\"x\",\"y\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
